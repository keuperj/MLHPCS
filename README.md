<img src="MLHPCS.png">

# MLHPCS Workshops
**Machine Learning on HPC Systems** is workshop series held in conjunction with  [ISC Supercomputing Conference](https://www.isc-hpc.com/)

* **2020**: papers and videos of the talks given at MLHPCS'20 can be found [here](2020/README.md)


# MLHPCS 2021
* the second workshop on **Machine Learning on HPC Systems** is an online event, held in conjunction with the [ISC 2021](https://www.isc-hpc.com/) (**July 2nd**).


## Abstract
Over the last few years, Machine Learning (ML) - and in particular Deep Learning (DL) - has become an important research topic in the High Performance Computing (HPC) community. This comes along with new users and data intensive applications on HPC systems, which increasingly affects the design and operation of compute infrastructures. HPC environment and resources on the one hand provide opportunities to attack ML/DL problems not tractable otherwise. On the other hand, the ML/DL community is just getting started to utilize the performance of HPC, leaving out many opportunities for better parallelization and scalability. The intent of this workshop is to bring together researchers and practitioners from all communities to discuss three key topics in the context of High Performance Computing and ML/DL methods: parallelization and scaling of ML / DL algorithms, ML/DL applications on HPC systems, and HPC systems design and optimization for ML / DL workloads.

### Date: in conjunction with the [ISC 2021](https://www.isc-hpc.com/) online conference 
* ISC'21 is held from June 24th until July 2nd
* Workshop date: July 2nd 

### Topics / Scope
The aim of the workshop is to provide a platform for technical discussions and the presentation of work in progress, as well as, unsolved problems, which is complementary to the “Machine Learning Day” in the main conference program.

* Unsolved problems in ML / DL on HPC systems
* Scalable ML / DL algorithms
* Parallelization techniques 
* Libraries for ML / DL
* Tools + workflows for ML / DL on HPC systems
* Optimized HPC systems design and setup for efficient ML / DL 
* ML / DL applications on HPC Systems 

## Call for Paper
MLPCS '21 has a two stage submission process: submit a 1-2 page extended abstract to contribute with a talk to MLHPCS. Additionally, authors optionally can submit a post-conference paper for publication in LNCS.

### Abstract submission
* Extended astracts are due by **NEW DEADLINE: May 14th** (end of day anywhere on earth)
* Please use the [LNCS autor kit](https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines) to write your abstract and paper 
* [Submit abstract via CMT](https://cmt3.research.microsoft.com/MLHPCS2021)

### Full paper
* MLHPCS paper will be published in the Springer LNCS post Conference Proceedings of the ISC.
* Full papers of accepted abstracs are due by **July 10th** and will undergo a full review process. 
* Submit full paper via abstract in CMT
* Format: 18 pages LNCS format. User the [template from Springer](https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines )

## Program
Abtract will follow, title might change.

### Keynote: Surprises in Deep Learning Training: Symmetry, Efficiency, and Scale
Daniel Soudry,  Technion

[[abstract](2021/keynote.md)]

### Invitted Talk: Impact of large-scale pre-training on intra- and inter-domain transfer learning in full and few-shot regimes
Jenia Jitsev, Juelich Supercomputer Center, Helmholtz AI, Research Center Juelich

[[abstract](2021/invited_1.md)]

### Invitted Talk: Large-scale Neural Solvers for Partial Differential Equations
Nico Hoffmann, TU Dresden

[[abstract](2021/invited_2.md)]

### Invitted Talk: Deep Learning Meets Optimal Control - How Optimal Control Enables Faster and Better Training
Stefanie Günther, LLNL

[[abstract](2021/invited_3.md)]

### Invitted Talk: Challenges when scalling DL training to thousands on GPUs and TPUs
Ahmed Elnaggar, TU Munich

[[abstract](2021/invited_4.md)]

### Contributed Tak: MSM: Multi-Stage Multicuts forScalable Image Clustering
Kalun Ho (Fraunhofer ITWM); Avraam chatzimichailidis (Fraunhofer ITWM ); Margret Keuper (University of Mannheim); Janis Keuper (hs-offenburg)

[[abstract](2021/contributed_1.md)]

### Contributed Taks: Analysis of Black-box Optimization Algorithms to Tune TensorFlow's CPU Backend
Niranjan Hasabnis (Intel); Derssie Mebratu (Intel)

[[abstract](2021/contributed_2.md)]

### Contributed Tak: Parallel/distributed intelligent hyperparameters search for generative artificial neural networks
Mathias Esteban (Universidad de la República); Jamal Toutouh (Universidad de Málaga); Sergio Nesmachnow (Universidad de la República)

[[abstract](2021/contributed_3.md)]

### Contributed Taks: Machine learning for generic energy models of high performance computing
Jonathan Muraña (Universidad de la República); Carmen Navarrete (Leibniz Supercomputing Center); Sergio Nesmachnow (Universidad de la República)

[[abstract](2021/contributed_4.md)]

### Contributed Tak: Hyper-parameter optimisation on HPC – a comparative study
Peter Winkler (TU Dresden); Norman Koch (TU Dresden)

[[abstract](2021/contributed_5.md)]


## Contact
info@mlhpcs.org

## Organizing Committee
* Juan J. Durillo (LRZ, Munich)
* Dennis Hoppe (HLRS, Stuttgart)
* Jenia Jitsev (JSC, Jülich)
* Janis Keuper (IMLA / Fraunhofer ITWM)
* Sunna Torge (ZIH, Dresden)



